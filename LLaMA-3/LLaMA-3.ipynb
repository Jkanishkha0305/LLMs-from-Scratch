{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ef10ea",
   "metadata": {},
   "source": [
    "## Building LLaMA 3 LLM From Scratch using Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df38272",
   "metadata": {},
   "source": [
    "### 1. Setting the Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece tiktoken torch blobfile matplotlib huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the `notebook_login` function from the `huggingface_hub` module.\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Execute the `notebook_login` function to log in to the Hugging Face Hub.\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5327110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary function from the huggingface_hub library\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Define the repository information\n",
    "repo_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "subfolder = \"original\"  # Specify the subfolder within the repository\n",
    "\n",
    "# List of filenames to download\n",
    "filenames = [\"params.json\", \"tokenizer.model\", \"consolidated.00.pth\"] \n",
    "\n",
    "# Specify the directory where you want to save the downloaded files\n",
    "save_directory = \"llama-3-8B/\"  # Replace with your desired path\n",
    "\n",
    "# Download each file\n",
    "for filename in filenames:\n",
    "    hf_hub_download(\n",
    "        repo_id=repo_id,       # Repository ID\n",
    "        filename=filename,     # Name of the file to download\n",
    "        subfolder=subfolder,   # Subfolder within the repository\n",
    "        local_dir=save_directory  # Directory to save the downloaded file\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca68c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File system paths\n",
    "from pathlib import Path\n",
    "\n",
    "# Tokenization library\n",
    "import tiktoken\n",
    "\n",
    "# BPE loading function\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "\n",
    "# PyTorch library\n",
    "import torch\n",
    "\n",
    "# JSON handling\n",
    "import json\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad7d701",
   "metadata": {},
   "source": [
    "### 2. Understanding the File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214bac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the tokenizer from llama-3-8B\n",
    "tokenizer_model = load_tiktoken_bpe(\"/kaggle/working/llama-3-8B/original/tokenizer.model\")\n",
    "\n",
    "# Get the length of the tokenizer model \n",
    "len(tokenizer_model)\n",
    "# OUTPUT: 128000\n",
    "\n",
    "# Get the type of the `tokenizer_model` object.\n",
    "type(tokenizer_model)\n",
    "# OUTPUT: dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the first 10 items of tokenizer model\n",
    "dict(list(tokenizer_model.items())[5600:5610])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a PyTorch model of LLaMA-3-8B\n",
    "model = torch.load(\"/kaggle/working/llama-3-8B/original/consolidated.00.pth\")\n",
    "\n",
    "# printing first 11 layers of the architecture\n",
    "list(model.keys())[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the parameters JSON file\n",
    "with open(\"/kaggle/working/llama-3-8B/original/params.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Printing the content\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a3e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension\n",
    "dim = config[\"dim\"]\n",
    "\n",
    "# Layers\n",
    "n_layers = config[\"n_layers\"]\n",
    "\n",
    "# Heads\n",
    "n_heads = config[\"n_heads\"]\n",
    "\n",
    "# KV_heads\n",
    "n_kv_heads = config[\"n_kv_heads\"]\n",
    "\n",
    "# Vocabulary\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "\n",
    "# Multiple\n",
    "multiple_of = config[\"multiple_of\"]\n",
    "\n",
    "# Multiplier\n",
    "ffn_dim_multiplier = config[\"ffn_dim_multiplier\"]\n",
    "\n",
    "# Epsilon\n",
    "norm_eps = config[\"norm_eps\"]\n",
    "\n",
    "# RoPE\n",
    "rope_theta = torch.tensor(config[\"rope_theta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c7d07",
   "metadata": {},
   "source": [
    "### 3. Tokenizing our input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    \"<|begin_of_text|>\",  # Marks the beginning of a text sequence.\n",
    "    \"<|end_of_text|>\",  # Marks the end of a text sequence.\n",
    "    \"<|reserved_special_token_0|>\",  # Reserved for future use.\n",
    "    \"<|reserved_special_token_1|>\",  # Reserved for future use.\n",
    "    \"<|reserved_special_token_2|>\",  # Reserved for future use.\n",
    "    \"<|reserved_special_token_3|>\",  # Reserved for future use.\n",
    "    \"<|start_header_id|>\",  # Indicates the start of a header ID.\n",
    "    \"<|end_header_id|>\",  # Indicates the end of a header ID.\n",
    "    \"<|reserved_special_token_4|>\",  # Reserved for future use.\n",
    "    \"<|eot_id|>\",  # Marks the end of a turn (in a conversational context).\n",
    "] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)]  # A large set of tokens reserved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns based on which text will be break into tokens\n",
    "tokenize_breaker = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer with specified parameters\n",
    "tokenizer = tiktoken.Encoding(\n",
    "\n",
    "    # make sure to set path to tokenizer.model file\n",
    "    name = \"/kaggle/working/llama-3-8B/original/tokenizer.model\",\n",
    "\n",
    "    # Define tokenization pattern string\n",
    "    pat_str = tokenize_breaker,\n",
    "\n",
    "    # Assign BPE mergeable ranks from tokenizer_model of LLaMA-3\n",
    "    mergeable_ranks = tokenizer_model,\n",
    "\n",
    "    # Set special tokens with indices\n",
    "    special_tokens={token: len(tokenizer_model) + i for i, token in enumerate(special_tokens)},\n",
    ")\n",
    "\n",
    "# Encode \"hello world!\" and decode tokens to string\n",
    "tokenizer.decode(tokenizer.encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input prompt\n",
    "prompt = \"the answer to the ultimate question of life, the universe, and everything is \"\n",
    "\n",
    "# Encode the prompt using the tokenizer and prepend a special token (128000)\n",
    "tokens = [128000] + tokenizer.encode(prompt)\n",
    "\n",
    "print(tokens)  # Print the encoded tokens\n",
    "\n",
    "# Convert the list of tokens into a PyTorch tensor\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "# Decode each token back into its corresponding string\n",
    "prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens]\n",
    "\n",
    "print(prompt_split_as_tokens)  # Print the decoded tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462a826",
   "metadata": {},
   "source": [
    "### 4. Creating Embedding for each Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda83ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking dimension of input vector and embedding vector from llama-3 architecture\n",
    "print(dim, len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a068e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding layer with vocab size and embedding dimension\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, dim)\n",
    "\n",
    "# Copy pre-trained token embeddings to the embedding layer\n",
    "embedding_layer.weight.data.copy_(model[\"tok_embeddings.weight\"])\n",
    "\n",
    "# Get token embeddings for given tokens, converting to torch.bfloat16 format\n",
    "token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)\n",
    "\n",
    "# Print shape of resulting token embeddings\n",
    "token_embeddings_unnormalized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e513bcf",
   "metadata": {},
   "source": [
    "### 5. Normalization Using RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating RMSNorm\n",
    "def rms_norm(tensor, norm_weights):\n",
    "\n",
    "    # Calculate the mean of the square of tensor values along the last dimension\n",
    "    squared_mean = tensor.pow(2).mean(-1, keepdim=True)\n",
    "    \n",
    "    # Add a small value to avoid division by zero\n",
    "    normalized = torch.rsqrt(squared_mean + norm_eps)\n",
    "    \n",
    "    # Multiply normalized tensor by the provided normalization weights\n",
    "    return (tensor * normalized) * norm_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using RMS normalization and provided normalization weights\n",
    "token_embeddings = rms_norm(token_embeddings_unnormalized, \n",
    "                            model[\"layers.0.attention_norm.weight\"])\n",
    "\n",
    "# Print the shape of the resulting token embeddings\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8d0c6",
   "metadata": {},
   "source": [
    "### 6. Attention Heads (Query, Key, Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of different weights\n",
    "print(\n",
    "    # Query weight shape\n",
    "    model[\"layers.0.attention.wq.weight\"].shape,\n",
    "    \n",
    "    # Key weight shape\n",
    "    model[\"layers.0.attention.wk.weight\"].shape,\n",
    "    \n",
    "    # Value weight shape\n",
    "    model[\"layers.0.attention.wv.weight\"].shape,\n",
    "    \n",
    "    # Output weight shape\n",
    "    model[\"layers.0.attention.wo.weight\"].shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd010e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve query weight for the first layer of attention\n",
    "q_layer0 = model[\"layers.0.attention.wq.weight\"]\n",
    "\n",
    "# Calculate dimension per head\n",
    "head_dim = q_layer0.shape[0] // n_heads\n",
    "\n",
    "# Reshape query weight to separate heads\n",
    "q_layer0 = q_layer0.view(n_heads, head_dim, dim)\n",
    "\n",
    "# Print the shape of the reshaped query weight tensor\n",
    "q_layer0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaeaf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the query weight for the first head of the first layer of attention\n",
    "q_layer0_head0 = q_layer0[0]\n",
    "\n",
    "# Print the shape of the extracted query weight tensor for the first head\n",
    "q_layer0_head0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0dabe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication: token embeddings with transpose of query weight for first head\n",
    "q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T)\n",
    "\n",
    "# Shape of resulting tensor: queries per token\n",
    "q_per_token.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d333c5d",
   "metadata": {},
   "source": [
    "### 7. Implementing RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1118c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert queries per token to float and split into pairs\n",
    "q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)\n",
    "\n",
    "# Print the shape of the resulting tensor after splitting into pairs\n",
    "q_per_token_split_into_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d9443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate values from 0 to 1 split into 64 parts\n",
    "zero_to_one_split_into_64_parts = torch.tensor(range(64))/64\n",
    "\n",
    "# Print the resulting tensor\n",
    "zero_to_one_split_into_64_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670518cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate frequencies using a power operation\n",
    "freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)\n",
    "\n",
    "# Display the resulting frequencies\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert queries per token to complex numbers\n",
    "q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)\n",
    "\n",
    "q_per_token_as_complex_numbers.shape\n",
    "# Output: torch.Size([17, 64])\n",
    "\n",
    "# Calculate frequencies for each token using outer product of arange(17) and freqs\n",
    "freqs_for_each_token = torch.outer(torch.arange(17), freqs)\n",
    "\n",
    "# Calculate complex numbers from frequencies_for_each_token using polar coordinates\n",
    "freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)\n",
    "\n",
    "# Rotate complex numbers by frequencies\n",
    "q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis\n",
    "\n",
    "q_per_token_as_complex_numbers_rotated.shape\n",
    "# Output: torch.Size([17, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e04ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rotated complex numbers back to real numbers\n",
    "q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "q_per_token_split_into_pairs_rotated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c355a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape rotated token queries to match the original shape\n",
    "q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "q_per_token_rotated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1421a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weight tensor for the attention mechanism's key in the first layer of the model\n",
    "k_layer0 = model[\"layers.0.attention.wk.weight\"]\n",
    "\n",
    "# Reshape key weight for the first layer of attention to separate heads\n",
    "k_layer0 = k_layer0.view(n_kv_heads, k_layer0.shape[0] // n_kv_heads, dim)\n",
    "\n",
    "# Print the shape of the reshaped key weight tensor\n",
    "k_layer0.shape  # Output: torch.Size([8, 128, 4096])\n",
    "\n",
    "# Extract the key weight for the first head of the first layer of attention\n",
    "k_layer0_head0 = k_layer0[0]\n",
    "\n",
    "# Print the shape of the extracted key weight tensor for the first head\n",
    "k_layer0_head0.shape  # Output: torch.Size([128, 4096])\n",
    "\n",
    "# Calculate key per token by matrix multiplication\n",
    "k_per_token = torch.matmul(token_embeddings, k_layer0_head0.T)\n",
    "\n",
    "# Print the shape of the resulting tensor representing keys per token\n",
    "k_per_token.shape  # Output: torch.Size([17, 128])\n",
    "\n",
    "# Split key per token into pairs and convert to float\n",
    "k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)\n",
    "\n",
    "# Print the shape of the resulting tensor after splitting into pairs\n",
    "k_per_token_split_into_pairs.shape  # Output: torch.Size([17, 64, 2])\n",
    "\n",
    "# Convert key per token to complex numbers\n",
    "k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)\n",
    "\n",
    "# Print the shape of the resulting tensor representing key per token as complex numbers\n",
    "k_per_token_as_complex_numbers.shape  # Output: torch.Size([17, 64])\n",
    "\n",
    "# Rotate complex key per token by frequencies\n",
    "k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)\n",
    "\n",
    "# Print the shape of the rotated complex key per token\n",
    "k_per_token_split_into_pairs_rotated.shape  # Output: torch.Size([17, 64, 2])\n",
    "\n",
    "# Reshape rotated key per token to match the original shape\n",
    "k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)\n",
    "\n",
    "# Print the shape of the rotated key per token\n",
    "k_per_token_rotated.shape  # Output: torch.Size([17, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59781c2",
   "metadata": {},
   "source": [
    "### 8. Implementing Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3032cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate query-key dot products per token\n",
    "qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T) / (head_dim) ** 0.5\n",
    "\n",
    "# Print the shape of the resulting tensor representing query-key dot products per token\n",
    "qk_per_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b32b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask tensor filled with negative infinity values\n",
    "mask = torch.full((len(tokens), len(tokens)), float(\"-inf\"), device=tokens.device)\n",
    "\n",
    "# Set upper triangular part of the mask tensor to negative infinity\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "# Print the resulting mask tensor\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef52c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the mask to the query-key dot products per token\n",
    "qk_per_token_after_masking = qk_per_token + mask\n",
    "\n",
    "# Apply softmax along the second dimension after masking\n",
    "qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95968cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the value weight for the first layer of attention\n",
    "v_layer0 = model[\"layers.0.attention.wv.weight\"]\n",
    "\n",
    "# Reshape value weight for the first layer of attention to separate heads\n",
    "v_layer0 = v_layer0.view(n_kv_heads, v_layer0.shape[0] // n_kv_heads, dim)\n",
    "\n",
    "# Print the shape of the reshaped value weight tensor\n",
    "v_layer0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the value weight for the first head of the first layer of attention\n",
    "v_layer0_head0 = v_layer0[0]\n",
    "\n",
    "# Print the shape of the extracted value weight tensor for the first head\n",
    "v_layer0_head0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90267a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate value per token by matrix multiplication\n",
    "v_per_token = torch.matmul(token_embeddings, v_layer0_head0.T)\n",
    "\n",
    "# Print the shape of the resulting tensor representing values per token\n",
    "v_per_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db370868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate QKV attention by matrix multiplication\n",
    "qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "qkv_attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9677e2ee",
   "metadata": {},
   "source": [
    "### 9. Implementing Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c5f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store QKV attention for each head in a list\n",
    "qkv_attention_store = []\n",
    "\n",
    "# Iterate through each head\n",
    "for head in range(n_heads):\n",
    "    # Extract query, key, and value weights for the current head\n",
    "    q_layer0_head = q_layer0[head]\n",
    "    k_layer0_head = k_layer0[head//4]  # Key weights are shared across 4 heads\n",
    "    v_layer0_head = v_layer0[head//4]  # Value weights are shared across 4 heads\n",
    "    \n",
    "    # Calculate query per token by matrix multiplication\n",
    "    q_per_token = torch.matmul(token_embeddings, q_layer0_head.T)\n",
    "    \n",
    "    # Calculate key per token by matrix multiplication\n",
    "    k_per_token = torch.matmul(token_embeddings, k_layer0_head.T)\n",
    "    \n",
    "    # Calculate value per token by matrix multiplication\n",
    "    v_per_token = torch.matmul(token_embeddings, v_layer0_head.T)\n",
    "    \n",
    "    # Split query per token into pairs and rotate them\n",
    "    q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)\n",
    "    q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)\n",
    "    q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis[:len(tokens)])\n",
    "    q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)\n",
    "    \n",
    "    # Split key per token into pairs and rotate them\n",
    "    k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)\n",
    "    k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)\n",
    "    k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis[:len(tokens)])\n",
    "    k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)\n",
    "    \n",
    "    # Calculate query-key dot products per token\n",
    "    qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T) / (128) ** 0.5\n",
    "    \n",
    "    # Create a mask tensor filled with negative infinity values\n",
    "    mask = torch.full((len(tokens), len(tokens)), float(\"-inf\"), device=tokens.device)\n",
    "    # Set upper triangular part of the mask tensor to negative infinity\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    # Add the mask to the query-key dot products per token\n",
    "    qk_per_token_after_masking = qk_per_token + mask\n",
    "    \n",
    "    # Apply softmax along the second dimension after masking\n",
    "    qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)\n",
    "    \n",
    "    # Calculate QKV attention by matrix multiplication\n",
    "    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)\n",
    "    \n",
    "    # Store QKV attention for the current head\n",
    "    qkv_attention_store.append(qkv_attention)\n",
    "\n",
    "# Print the number of QKV attentions stored\n",
    "len(qkv_attention_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a053e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate QKV attentions from all heads along the last dimension\n",
    "stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "stacked_qkv_attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1572f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the embedding delta by matrix multiplication with the output weight\n",
    "embedding_delta = torch.matmul(stacked_qkv_attention, model[\"layers.0.attention.wo.weight\"].T)\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "embedding_delta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae48e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the embedding delta to the unnormalized token embeddings to get the final embeddings\n",
    "embedding_after_edit = token_embeddings_unnormalized + embedding_delta\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "embedding_after_edit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize edited embeddings using root mean square normalization and provided weights\n",
    "embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[\"layers.0.ffn_norm.weight\"])\n",
    "\n",
    "# Print the shape of resulting normalized embeddings\n",
    "embedding_after_edit_normalized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5042cd7",
   "metadata": {},
   "source": [
    "### 10. Implementing SwiGLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a9e250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve weights for feedforward layer\n",
    "w1 = model[\"layers.0.feed_forward.w1.weight\"]\n",
    "w2 = model[\"layers.0.feed_forward.w2.weight\"]\n",
    "w3 = model[\"layers.0.feed_forward.w3.weight\"]\n",
    "\n",
    "# Perform operations for feedforward layer\n",
    "output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)\n",
    "\n",
    "# Print the shape of the resulting tensor after feedforward\n",
    "output_after_feedforward.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde5857",
   "metadata": {},
   "source": [
    "### 11. Merging everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize final embedding with unnormalized token embeddings\n",
    "final_embedding = token_embeddings_unnormalized\n",
    "\n",
    "# Iterate through each layer\n",
    "for layer in range(n_layers):\n",
    "    # Initialize list to store QKV attentions for each head\n",
    "    qkv_attention_store = []\n",
    "    \n",
    "    # Normalize the final embedding using root mean square normalization and weights from the current layer\n",
    "    layer_embedding_norm = rms_norm(final_embedding, model[f\"layers.{layer}.attention_norm.weight\"])\n",
    "    \n",
    "    # Retrieve query, key, value, and output weights for the attention mechanism of the current layer\n",
    "    q_layer = model[f\"layers.{layer}.attention.wq.weight\"]\n",
    "    q_layer = q_layer.view(n_heads, q_layer.shape[0] // n_heads, dim)\n",
    "    k_layer = model[f\"layers.{layer}.attention.wk.weight\"]\n",
    "    k_layer = k_layer.view(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)\n",
    "    v_layer = model[f\"layers.{layer}.attention.wv.weight\"]\n",
    "    v_layer = v_layer.view(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)\n",
    "    w_layer = model[f\"layers.{layer}.attention.wo.weight\"]\n",
    "    \n",
    "    # Iterate through each head\n",
    "    for head in range(n_heads):\n",
    "        # Extract query, key, and value weights for the current head\n",
    "        q_layer_head = q_layer[head]\n",
    "        k_layer_head = k_layer[head//4]  # Key weights are shared across 4 heads\n",
    "        v_layer_head = v_layer[head//4]  # Value weights are shared across 4 heads\n",
    "        \n",
    "        # Calculate query per token by matrix multiplication\n",
    "        q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T)\n",
    "        \n",
    "        # Calculate key per token by matrix multiplication\n",
    "        k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T)\n",
    "        \n",
    "        # Calculate value per token by matrix multiplication\n",
    "        v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T)\n",
    "        \n",
    "        # Split query per token into pairs and rotate them\n",
    "        q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)\n",
    "        q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)\n",
    "        q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis)\n",
    "        q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)\n",
    "        \n",
    "        # Split key per token into pairs and rotate them\n",
    "        k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)\n",
    "        k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)\n",
    "        k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)\n",
    "        k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)\n",
    "        \n",
    "        # Calculate query-key dot products per token\n",
    "        qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T) / (128) ** 0.5\n",
    "        \n",
    "        # Create a mask tensor filled with negative infinity values\n",
    "        mask = torch.full((len(token_embeddings_unnormalized), len(token_embeddings_unnormalized)), float(\"-inf\"))\n",
    "        # Set upper triangular part of the mask tensor to negative infinity\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "        # Add the mask to the query-key dot products per token\n",
    "        qk_per_token_after_masking = qk_per_token + mask\n",
    "        \n",
    "        # Apply softmax along the second dimension after masking\n",
    "        qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)\n",
    "        \n",
    "        # Calculate QKV attention by matrix multiplication\n",
    "        qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)\n",
    "        \n",
    "        # Store QKV attention for the current head\n",
    "        qkv_attention_store.append(qkv_attention)\n",
    "    \n",
    "    # Concatenate QKV attentions from all heads along the last dimension\n",
    "    stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)\n",
    "    \n",
    "    # Calculate embedding delta by matrix multiplication with the output weight\n",
    "    embedding_delta = torch.matmul(stacked_qkv_attention, w_layer.T)\n",
    "    \n",
    "    # Add the embedding delta to the current embedding to get the edited embedding\n",
    "    embedding_after_edit = final_embedding + embedding_delta\n",
    "    \n",
    "    # Normalize the edited embedding using root mean square normalization and weights from the current layer\n",
    "    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f\"layers.{layer}.ffn_norm.weight\"])\n",
    "    \n",
    "    # Retrieve weights for the feedforward layer\n",
    "    w1 = model[f\"layers.{layer}.feed_forward.w1.weight\"]\n",
    "    w2 = model[f\"layers.{layer}.feed_forward.w2.weight\"]\n",
    "    w3 = model[f\"layers.{layer}.feed_forward.w3.weight\"]\n",
    "    \n",
    "    # Perform operations for the feedforward layer\n",
    "    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)\n",
    "    \n",
    "    # Update the final embedding with the edited embedding plus the output from the feedforward layer\n",
    "    final_embedding = embedding_after_edit + output_after_feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0354596e",
   "metadata": {},
   "source": [
    "### 12. Generating the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the final embedding using root mean square normalization and provided weights\n",
    "final_embedding = rms_norm(final_embedding, model[\"norm.weight\"])\n",
    "\n",
    "# Print the shape of the resulting normalized final embedding\n",
    "final_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d603d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the output weight tensor\n",
    "model[\"output.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85649eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate logits by matrix multiplication between the final embedding and the transpose of the output weight tensor\n",
    "logits = torch.matmul(final_embedding[-1], model[\"output.weight\"].T)\n",
    "\n",
    "# Find the index of the maximum value along the last dimension to determine the next token\n",
    "next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Decode the index of the next token using the tokenizer\n",
    "tokenizer.decode([next_token.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2091adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input prompt\n",
    "prompt = \"Your Input\"\n",
    "\n",
    "# Replacing 17 number with total number of tokens in your input\n",
    "# You can check total number of tokens using len(tokens)\n",
    "freqs_for_each_token = torch.outer(torch.arange(17), freqs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
