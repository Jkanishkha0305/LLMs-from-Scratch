{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8826c15a",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch for implementing LLM (No GPU)\n",
    "import torch\n",
    "\n",
    "# Neural network modules and functions from PyTorch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# NumPy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib for plotting Loss etc.\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Time module for tracking execution time\n",
    "import time\n",
    "\n",
    "# Pandas for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# urllib for handling URL requests (Downloading Dataset)\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a6cf22",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration object for model parameters\n",
    "MASTER_CONFIG = {\n",
    "    # Adding parameters later\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693e046a",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL of the raw text file on GitHub\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "# The file name for local storage\n",
    "file_name = \"tinyshakespeare.txt\"\n",
    "\n",
    "# Execute the download\n",
    "urllib.request.urlretrieve(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891bb90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the dataset\n",
    "lines = open(\"tinyshakespeare.txt\", 'r').read()\n",
    "\n",
    "# Create a sorted list of unique characters in the dataset\n",
    "vocab = sorted(list(set(lines)))\n",
    "\n",
    "# Display the first 10 characters in the vocabulary list\n",
    "print('Printing the first 10 characters of the vocab list:', vocab[:10])\n",
    "\n",
    "# Output the total number of characters in our dataset (Vocabulary Size)\n",
    "print('Total number of characters in our dataset (Vocabulary Size):', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc2454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping integers to characters (itos)\n",
    "itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "# Mapping characters to integers (stoi)\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17705635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode function: Converts a string to a list of integers using the mapping stoi\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "# Decode function: Converts a list of integers back to a string using the mapping itos\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Example: Encode the string \"hello\" and then decode the result\n",
    "decode(encode(\"morning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into a torch tensor with specified data type (dtype)\n",
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
    "\n",
    "# adding the vocab size\n",
    "MASTER_CONFIG = {\n",
    "    \"vocab_size\": len(vocab),\n",
    "}\n",
    "\n",
    "# Display the shape of the resulting tensor\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4065d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get batches for training, validation, or testing\n",
    "def get_batches(data, split, batch_size, context_window, config=MASTER_CONFIG):\n",
    "    # Split the dataset into training, validation, and test sets\n",
    "    train = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    test = data[int(.9 * len(data)):]\n",
    "\n",
    "    # Determine which split to use\n",
    "    batch_data = train\n",
    "    if split == 'val':\n",
    "        batch_data = val\n",
    "    if split == 'test':\n",
    "        batch_data = test\n",
    "\n",
    "    # Pick random starting points within the data\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "\n",
    "    # Create input sequences (x) and corresponding target sequences (y)\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c4001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the MASTER_CONFIG with batch_size and context_window parameters\n",
    "MASTER_CONFIG.update({\n",
    "    'batch_size': 8,          # Number of batches to be processed at each random split\n",
    "    'context_window': 16      # Number of characters in each input (x) and target (y) sequence of each batch\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be895d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain batches for training using the specified batch size and context window\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Decode the sequences to obtain the corresponding text representations\n",
    "decoded_samples = [(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))]\n",
    "\n",
    "# Print the random sample\n",
    "print(decoded_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40340b",
   "metadata": {},
   "source": [
    "Evaluation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5adece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # Don't compute gradients for this function\n",
    "def evaluate_loss(model, config=MASTER_CONFIG):\n",
    "    # Placeholder for the evaluation results\n",
    "    out = {}\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate through training and validation splits\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        # Placeholder for individual losses\n",
    "        losses = []\n",
    "\n",
    "        # Generate 10 batches for evaluation\n",
    "        for _ in range(10):\n",
    "            # Get input sequences (xb) and target sequences (yb)\n",
    "            xb, yb = get_batches(dataset, split, config['batch_size'], config['context_window'])\n",
    "            \n",
    "            # Perform model inference and calculate the loss\n",
    "            _, loss = model(xb, yb)\n",
    "            \n",
    "            # Append the loss to the list\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Calculate the mean loss for the split and store it in the output dictionary\n",
    "        out[split] = np.mean(losses)\n",
    "    \n",
    "    # Set the model back to training mode\n",
    "    model.train()\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8266419",
   "metadata": {},
   "source": [
    "### 3. Setting up a Base Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01500dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of a basic neural network class\n",
    "class SimpleBrokenModel(nn.Module):\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding layer to convert character indices to vectors (vocab size: 65)\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "\n",
    "        # Linear layers for modeling relationships between features\n",
    "        # (to be updated with SwiGLU activation function as in LLaMA)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            nn.ReLU(),  # Currently using ReLU, will be replaced with SwiGLU as in LLaMA\n",
    "            nn.Linear(config['d_model'], config['vocab_size']),\n",
    "        )\n",
    "\n",
    "        # Print the total number of model parameters\n",
    "        print(\"Model parameters:\", sum([m.numel() for m in self.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc370c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of a basic neural network class\n",
    "class SimpleBrokenModel(nn.Module):\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding layer to convert character indices to vectors (vocab size: 65)\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "\n",
    "        # Linear layers for modeling relationships between features\n",
    "        # (to be updated with SwiGLU activation function as in LLaMA)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            nn.ReLU(),  # Currently using ReLU, will be replaced with SwiGLU as in LLaMA\n",
    "            nn.Linear(config['d_model'], config['vocab_size']),\n",
    "        )\n",
    "\n",
    "        # Print the total number of model parameters\n",
    "        print(\"Model parameters:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    # Forward pass function for the base model\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Embedding layer converts character indices to vectors\n",
    "        x = self.embedding(idx)\n",
    "        \n",
    "        # Linear layers for modeling relationships between features\n",
    "        a = self.linear(x)\n",
    "        \n",
    "        # Apply softmax activation to obtain probability distribution\n",
    "        logits = F.softmax(a, dim=-1)\n",
    "\n",
    "        # If targets are provided, calculate and return the cross-entropy loss\n",
    "        if targets is not None:\n",
    "            # Reshape logits and targets for cross-entropy calculation\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "        # If targets are not provided, return the logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6688983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update MASTER_CONFIG with the dimension of linear layers (128)\n",
    "MASTER_CONFIG.update({\n",
    "    'd_model': 128,\n",
    "})\n",
    "\n",
    "# Instantiate the SimpleBrokenModel using the updated MASTER_CONFIG\n",
    "model = SimpleBrokenModel(MASTER_CONFIG)\n",
    "\n",
    "# Print the total number of parameters in the model\n",
    "print(\"Total number of parameters in the Simple Neural Network Model:\", sum([m.numel() for m in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae56686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain batches for training using the specified batch size and context window\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Calculate logits and loss using the model\n",
    "logits, loss = model(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9609a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update MASTER_CONFIG with training parameters\n",
    "MASTER_CONFIG.update({\n",
    "    'epochs': 1000,          # Number of training epochs\n",
    "    'log_interval': 10,      # Log information every 10 batches during training\n",
    "    'batch_size': 32,        # Increase batch size to 32\n",
    "})\n",
    "\n",
    "# Instantiate the SimpleBrokenModel with updated configuration\n",
    "model = SimpleBrokenModel(MASTER_CONFIG)\n",
    "\n",
    "# Define the Adam optimizer for model parameters\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),      # Pass the model parameters to the optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8348234d",
   "metadata": {},
   "source": [
    "Let’s execute the training process and capture the loss from our base model, including the total number of parameters. **Additionally, each line is commented for clarity**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform training\n",
    "def train(model, optimizer, scheduler=None, config=MASTER_CONFIG, print_logs=False):\n",
    "    # Placeholder for storing losses\n",
    "    losses = []\n",
    "\n",
    "    # Start tracking time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through epochs\n",
    "    for epoch in range(config['epochs']):\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Obtain batches for training\n",
    "        xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
    "\n",
    "        # Forward pass through the model to calculate logits and loss\n",
    "        logits, loss = model(xs, targets=ys)\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # If a learning rate scheduler is provided, adjust the learning rate\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Log progress every specified interval\n",
    "        if epoch % config['log_interval'] == 0:\n",
    "            # Calculate batch time\n",
    "            batch_time = time.time() - start_time\n",
    "\n",
    "            # Evaluate loss on validation set\n",
    "            x = evaluate_loss(model)\n",
    "\n",
    "            # Store the validation loss\n",
    "            losses += [x]\n",
    "\n",
    "            # Print progress logs if specified\n",
    "            if print_logs:\n",
    "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config['epochs'] - epoch)/config['log_interval'] :.3f}\")\n",
    "\n",
    "            # Reset the timer\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Print learning rate if a scheduler is provided\n",
    "            if scheduler:\n",
    "                print(\"lr: \", scheduler.get_lr())\n",
    "\n",
    "    # Print the final validation loss\n",
    "    print(\"Validation loss: \", losses[-1]['val'])\n",
    "\n",
    "    # Plot the training and validation loss curves\n",
    "    return pd.DataFrame(losses).plot()\n",
    "\n",
    "# Execute the training process\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of a basic neural network class\n",
    "class SimpleBrokenModel(nn.Module):\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding layer to convert character indices to vectors (vocab size: 65)\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "\n",
    "        # Linear layers for modeling relationships between features\n",
    "        # (to be updated with SwiGLU activation function as in LLaMA)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            nn.ReLU(),  # Currently using ReLU, will be replaced with SwiGLU as in LLaMA\n",
    "            nn.Linear(config['d_model'], config['vocab_size']),\n",
    "        )\n",
    "\n",
    "        # Print the total number of model parameters\n",
    "        print(\"Model parameters:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    # Forward pass function for the base model\n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        # Embedding layer converts character indices to vectors\n",
    "        x = self.embedding(idx)\n",
    "        \n",
    "        # Linear layers for modeling relationships between features\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        # If targets are provided, calculate and return the cross-entropy loss\n",
    "        if targets is not None:\n",
    "            # Reshape logits and targets for cross-entropy calculation\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "        # If targets are not provided, return the logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the updated SimpleModel\n",
    "model = SimpleBrokenModel(MASTER_CONFIG)\n",
    "\n",
    "# Obtain batches for training\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Calculate logits and loss using the model\n",
    "logits, loss = model(xs, ys)\n",
    "\n",
    "# Define the Adam optimizer for model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model for 100 epochs\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e83452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate function for text generation using the trained model\n",
    "def generate(model, config=MASTER_CONFIG, max_new_tokens=30):\n",
    "    idx = torch.zeros(5, 1).long()\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Call the model\n",
    "        logits = model(idx[:, -config['context_window']:])\n",
    "        last_time_step_logits = logits[\n",
    "            :, -1, :\n",
    "        ]  # all the batches (1), last time step, all the logits\n",
    "        p = F.softmax(last_time_step_logits, dim=-1)  # softmax to get probabilities\n",
    "        idx_next = torch.multinomial(\n",
    "            p, num_samples=1\n",
    "        )  # sample from the distribution to get the next token\n",
    "        idx = torch.cat([idx, idx_next], dim=-1)  # append to the sequence\n",
    "    return [decode(x) for x in idx.tolist()]\n",
    "\n",
    "# Generate text using the trained model\n",
    "generate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad9cd1",
   "metadata": {},
   "source": [
    "### 4. Replicating LLaMA Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c9823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, layer_shape, eps=1e-8, bias=False):\n",
    "        super(RMSNorm, self).__init__()\n",
    "\n",
    "        # Registering a learnable parameter 'scale' as a parameter of the module\n",
    "        self.register_parameter(\"scale\", nn.Parameter(torch.ones(layer_shape)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Assumes shape is (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Calculating the Frobenius norm, RMS = 1/sqrt(N) * Frobenius norm\n",
    "        ff_rms = torch.linalg.norm(x, dim=(1,2)) * x[0].numel() ** -.5\n",
    "\n",
    "        # Normalizing the input tensor 'x' with respect to RMS\n",
    "        raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Scaling the normalized tensor using the learnable parameter 'scale'\n",
    "        return self.scale[:x.shape[1], :].unsqueeze(0) * raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SimpleModel_RMS with RMSNorm\n",
    "class SimpleModel_RMS(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding layer to convert character indices to vectors\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "\n",
    "        # RMSNorm layer for pre-normalization\n",
    "        self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
    "\n",
    "        # Linear layers for modeling relationships between features\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            nn.ReLU(),  # Currently using ReLU, will be replaced with SwiGLU as in LLaMA\n",
    "            nn.Linear(config['d_model'], config['vocab_size']),\n",
    "        )\n",
    "\n",
    "        # Print the total number of model parameters\n",
    "        print(\"Model parameters:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Embedding layer converts character indices to vectors\n",
    "        x = self.embedding(idx)\n",
    "\n",
    "        # RMSNorm pre-normalization\n",
    "        x = self.rms(x)\n",
    "\n",
    "        # Linear layers for modeling relationships between features\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        # If targets are provided, calculate and return the cross-entropy loss\n",
    "        if targets is not None:\n",
    "            # Reshape logits and targets for cross-entropy calculation\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "        # If targets are not provided, return the logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of SimpleModel_RMS\n",
    "model = SimpleModel_RMS(MASTER_CONFIG)\n",
    "\n",
    "# Obtain batches for training\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Calculate logits and loss using the model\n",
    "logits, loss = model(xs, ys)\n",
    "\n",
    "# Define the Adam optimizer for model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543d7d49",
   "metadata": {},
   "source": [
    "### 5. Rotary Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6df26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotary_matrix(context_window, embedding_dim):\n",
    "    # Initialize a tensor for the rotary matrix with zeros\n",
    "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "\n",
    "    # Loop through each position in the context window\n",
    "    for position in range(context_window):\n",
    "        # Loop through each dimension in the embedding\n",
    "        for i in range(embedding_dim // 2):\n",
    "            # Calculate the rotation angle (theta) based on the position and embedding dimension\n",
    "            theta = 10000. ** (-2. * (i - 1) / embedding_dim)\n",
    "            # Calculate the rotated matrix elements using sine and cosine functions\n",
    "            m_theta = position * theta\n",
    "            R[position, 2 * i, 2 * i] = np.cos(m_theta)\n",
    "            R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)\n",
    "            R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)\n",
    "            R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87bfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPEMaskedAttentionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Linear transformation for query\n",
    "        self.w_q = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
    "        # Linear transformation for key\n",
    "        self.w_k = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
    "        # Linear transformation for value\n",
    "        self.w_v = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
    "        # Obtain rotary matrix for positional embeddings\n",
    "        self.R = get_rotary_matrix(config['context_window'], config['d_model'])\n",
    "\n",
    "    def get_rotary_matrix(context_window, embedding_dim):\n",
    "        # Initialize a tensor for the rotary matrix with zeros\n",
    "        R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "        \n",
    "        # Loop through each position in the context window\n",
    "        for position in range(context_window):\n",
    "            # Loop through each dimension in the embedding\n",
    "            for i in range(embedding_dim // 2):\n",
    "                # Calculate the rotation angle (theta) based on the position and embedding dimension\n",
    "                theta = 10000. ** (-2. * (i - 1) / embedding_dim)\n",
    "                # Calculate the rotated matrix elements using sine and cosine functions\n",
    "                m_theta = position * theta\n",
    "                R[position, 2 * i, 2 * i] = np.cos(m_theta)\n",
    "                R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)\n",
    "                R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)\n",
    "                R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)\n",
    "        return R\n",
    "\n",
    "    def forward(self, x, return_attn_weights=False):\n",
    "        # x: input tensor of shape (batch, sequence length, dimension)\n",
    "\n",
    "        b, m, d = x.shape  # batch size, sequence length, dimension\n",
    "\n",
    "        # Linear transformations for Q, K, and V\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        # Rotate Q and K using the RoPE matrix\n",
    "        q_rotated = (torch.bmm(q.transpose(0, 1), self.R[:m])).transpose(0, 1)\n",
    "        k_rotated = (torch.bmm(k.transpose(0, 1), self.R[:m])).transpose(0, 1)\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        activations = F.scaled_dot_product_attention(\n",
    "            q_rotated, k_rotated, v, dropout_p=0.1, is_causal=True\n",
    "        )\n",
    "\n",
    "        if return_attn_weights:\n",
    "            # Create a causal attention mask\n",
    "            attn_mask = torch.tril(torch.ones((m, m)), diagonal=0)\n",
    "            # Calculate attention weights and add causal mask\n",
    "            attn_weights = torch.bmm(q_rotated, k_rotated.transpose(1, 2)) / np.sqrt(d) + attn_mask\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "            return activations, attn_weights\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPEMaskedMultiheadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Create a list of RoPEMaskedAttentionHead instances as attention heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            RoPEMaskedAttentionHead(config) for _ in range(config['n_heads'])\n",
    "        ])\n",
    "        self.linear = nn.Linear(config['n_heads'] * config['d_model'], config['d_model'])  # Linear layer after concatenating heads\n",
    "        self.dropout = nn.Dropout(.1)  # Dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input tensor of shape (batch, sequence length, dimension)\n",
    "\n",
    "        # Process each attention head and concatenate the results\n",
    "        heads = [h(x) for h in self.heads]\n",
    "        x = torch.cat(heads, dim=-1)\n",
    "        \n",
    "        # Apply linear transformation to the concatenated output\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9abb29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the master configuration with the number of attention heads\n",
    "MASTER_CONFIG.update({\n",
    "    'n_heads': 8,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b160d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RopeModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding layer for input tokens\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        \n",
    "        # RMSNorm layer for pre-normalization\n",
    "        self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
    "        \n",
    "        # RoPEMaskedMultiheadAttention layer\n",
    "        self.rope_attention = RoPEMaskedMultiheadAttention(config)\n",
    "\n",
    "        # Linear layer followed by ReLU activation\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Final linear layer for prediction\n",
    "        self.last_linear = nn.Linear(config['d_model'], config['vocab_size'])\n",
    "\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx: input indices\n",
    "        x = self.embedding(idx)\n",
    "\n",
    "        # One block of attention\n",
    "        x = self.rms(x)  # RMS pre-normalization\n",
    "        x = x + self.rope_attention(x)\n",
    "\n",
    "        x = self.rms(x)  # RMS pre-normalization\n",
    "        x = x + self.linear(x)\n",
    "\n",
    "        logits = self.last_linear(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of RopeModel (RMSNorm, RoPE, Multi-Head)\n",
    "model = RopeModel(MASTER_CONFIG)\n",
    "\n",
    "# Obtain batches for training\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Calculate logits and loss using the model\n",
    "logits, loss = model(xs, ys)\n",
    "\n",
    "# Define the Adam optimizer for model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c638551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating training configuration with more epochs and a logging interval\n",
    "MASTER_CONFIG.update({\n",
    "    \"epochs\": 5000,\n",
    "    \"log_interval\": 10,\n",
    "})\n",
    "\n",
    "# Training the model with the updated configuration\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af16941",
   "metadata": {},
   "source": [
    "### 6. SwiGLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece94501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Swish-Gated Linear Unit\n",
    "    https://arxiv.org/pdf/2002.05202v1.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.linear_gate = nn.Linear(size, size)\n",
    "        self.linear = nn.Linear(size, size)\n",
    "        self.beta = torch.randn(1, requires_grad=True)\n",
    "\n",
    "        self.beta = nn.Parameter(torch.ones(1))\n",
    "        self.register_parameter(\"beta\", self.beta)\n",
    "\n",
    "    def forward(self, x): \n",
    "        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))\n",
    "        out = swish_gate * self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884fdfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RopeModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding layer for input tokens\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        \n",
    "        # RMSNorm layer for pre-normalization\n",
    "        self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
    "        \n",
    "        # Multi-head attention layer with RoPE (Rotary Positional Embeddings)\n",
    "        self.rope_attention = RoPEMaskedMultiheadAttention(config)\n",
    "\n",
    "        # Linear layer followed by SwiGLU activation\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            SwiGLU(config['d_model']),  # Adding SwiGLU activation\n",
    "        )\n",
    "\n",
    "        # Output linear layer\n",
    "        self.last_linear = nn.Linear(config['d_model'], config['vocab_size'])\n",
    "\n",
    "        # Printing total model parameters\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embedding(idx)\n",
    "\n",
    "        # One block of attention\n",
    "        x = self.rms(x)  # RMS pre-normalization\n",
    "        x = x + self.rope_attention(x)\n",
    "\n",
    "        x = self.rms(x)  # RMS pre-normalization\n",
    "        x = x + self.linear(x)  # Applying SwiGLU activation\n",
    "\n",
    "        logits = self.last_linear(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # Calculate cross-entropy loss if targets are provided\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c8f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of RopeModel (RMSNorm, RoPE, Multi-Head, SwiGLU)\n",
    "model = RopeModel(MASTER_CONFIG)\n",
    "\n",
    "# Obtain batches for training\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Calculate logits and loss using the model\n",
    "logits, loss = model(xs, ys)\n",
    "\n",
    "# Define the Adam optimizer for model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155749e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model configurations for the number of layers\n",
    "MASTER_CONFIG.update({\n",
    "    'n_layers': 4,  # Set the number of layers to 4\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760daa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # RMSNorm layer\n",
    "        self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
    "\n",
    "        # RoPE Masked Multihead Attention layer\n",
    "        self.attention = RoPEMaskedMultiheadAttention(config)\n",
    "\n",
    "        # Feedforward layer with SwiGLU activation\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            SwiGLU(config['d_model']),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # one block of attention\n",
    "        x = self.rms(x) # RMS pre-normalization\n",
    "        x = x + self.attention(x)  # residual connection\n",
    "\n",
    "        x = self.rms(x) # RMS pre-normalization\n",
    "        x = x + self.feedforward(x)  # residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b477c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the LlamaBlock class with the provided configuration\n",
    "block = LlamaBlock(MASTER_CONFIG)\n",
    "\n",
    "# Generate a random tensor with the specified batch size, context window, and model dimension\n",
    "random_input = torch.randn(MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'], MASTER_CONFIG['d_model'])\n",
    "\n",
    "# Apply the LlamaBlock to the random input tensor\n",
    "output = block(random_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573cd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "MASTER_CONFIG.update({\n",
    "    'n_layers': 4,\n",
    "})\n",
    "\n",
    "class Llama(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Embedding layer for token representations\n",
    "        self.embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        # Sequential block of LlamaBlocks based on the specified number of layers\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"llama_{i}\", LlamaBlock(config)) for i in range(config['n_layers'])])\n",
    "        )\n",
    "        # Feedforward network (FFN) for final output\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            SwiGLU(config['d_model']),\n",
    "            nn.Linear(config['d_model'], config['vocab_size']),\n",
    "        )\n",
    "\n",
    "        # Print total number of parameters in the model\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Input token indices are passed through the embedding layer\n",
    "        x = self.embeddings(idx)\n",
    "        # Process the input through the LlamaBlocks\n",
    "        x = self.llama_blocks(x)\n",
    "        # Pass the processed input through the final FFN for output logits\n",
    "        logits = self.ffn(x)\n",
    "\n",
    "        # If targets are not provided, return only the logits\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        # If targets are provided, compute and return the cross-entropy loss\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d72cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of RopeModel (RMSNorm, RoPE, Multi-Head, SwiGLU, N_layers)\n",
    "llama = Llama(MASTER_CONFIG)\n",
    "\n",
    "# Obtain batches for training\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Calculate logits and loss using the model\n",
    "logits, loss = llama(xs, ys)\n",
    "\n",
    "# Define the Adam optimizer for model parameters\n",
    "optimizer = torch.optim.Adam(llama.parameters())\n",
    "\n",
    "# Train the model\n",
    "train(llama, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the number of epochs in the configuration\n",
    "MASTER_CONFIG.update({\n",
    "    'epochs': 10000,\n",
    "})\n",
    "# Train the LLaMA model for the specified number of epochs\n",
    "train(llama, optimizer, scheduler=None, config=MASTER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01669289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model again, scheduler for better optimization.\n",
    "train(llama, optimizer, config=MASTER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de156697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using the trained LLM (llama) with a maximum of 500 tokens\n",
    "generated_text = generate(llama, MASTER_CONFIG, 500)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batches from the test set\n",
    "xs, ys = get_batches(dataset, 'test', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Pass the test data through the LLaMA model\n",
    "logits, loss = llama(xs, ys)\n",
    "\n",
    "# Print the loss on the test set\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d980ba8e",
   "metadata": {},
   "source": [
    "### 7. Experimenting with Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4921705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update configuration\n",
    "MASTER_CONFIG.update({\n",
    "    \"epochs\": 1000\n",
    "})\n",
    "\n",
    "# Create Llama model with Cosine Annealing learning schedule\n",
    "llama_with_cosine = Llama(MASTER_CONFIG)\n",
    "\n",
    "# Define Adam optimizer with specific hyperparameters\n",
    "llama_optimizer = torch.optim.Adam(\n",
    "    llama.parameters(),\n",
    "    betas=(.9, .95),\n",
    "    weight_decay=.1,\n",
    "    eps=1e-9,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# Define Cosine Annealing learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(llama_optimizer, 300, eta_min=1e-5)\n",
    "\n",
    "# Train the Llama model with the specified optimizer and scheduler\n",
    "train(llama_with_cosine, llama_optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66071b0f",
   "metadata": {},
   "source": [
    "### 8. Saving Your Language Model (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb195072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "torch.save(llama, 'llama_model.pth')\n",
    "\n",
    "# If you want to save only the model parameters\n",
    "torch.save(llama.state_dict(), 'llama_model_params.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcdfb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# Assuming Llama is your PyTorch model\n",
    "llama_config = GPT2Config.from_dict(MASTER_CONFIG)\n",
    "llama_transformers = GPT2LMHeadModel(config=llama_config)\n",
    "llama_transformers.load_state_dict(llama.state_dict())\n",
    "\n",
    "# Specify the directory where you want to save the model\n",
    "output_dir = \"llama_model_transformers\"\n",
    "\n",
    "# Save the model and configuration\n",
    "llama_transformers.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56acb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# Specify the directory where the model was saved\n",
    "output_dir = \"llama_model_transformers\"\n",
    "\n",
    "# Load the model and configuration\n",
    "llama_transformers = GPT2LMHeadModel.from_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
